# Why you shouldn't match on treatment affected variables

This repository accompanies a [post](https://www.linkedin.com/pulse/simulation-did-psm-nate-ives-83are/?trackingId=aI%2BVnMr5Sh%2Btt6bzd9H0Tg%3D%3D) that I wrote with Nate Ives drawn from a project we worked on at USAID. The code demonstrates the bias introduced when post-treatment variables are included in propensity score matching models. The simulation compares two approaches for estimating causal effects in a difference-in-differences framework: an improper PSM approach that conditions on treatment-affected outcomes versus a doubly robust approach that uses only pre-treatment covariates.

The simulation generates panel data for 10,000 individuals across four time periods, with treatment assignment occurring after period 2 and a true treatment effect of 2.0 in the final period. The improper approach matches treated and control units based on both baseline characteristics and outcomes measured at time 2 (after treatment assignment), while the doubly robust approach uses inverse probability weighting based solely on pre-treatment variables. Through 10,000 simulation runs, the results clearly demonstrate that including post-treatment variables in the propensity score model introduces significant bias, as these variables are themselves affected by treatment assignment.

The core issue stems from conditioning on outcomes of the treatment itself, which creates artificial selection and distorts the comparison between treated and control groups. When researchers match on treatment-affected variables, they inadvertently select subgroups where treatment had specific effects on intermediate outcomes, rather than maintaining the random assignment structure necessary for valid causal inference. This violates a fundamental principle of causal analysis: only pre-treatment covariates should be used for matching or propensity score estimation to preserve the causal interpretation of treatment effects. The simulation results, visualized through density plots of treatment effect estimates, show that while the doubly robust method produces unbiased estimates centered around the true effect, the improper PSM approach yields systematically biased results that can mislead policy conclusions.

Our write-up and the simulation don't even get into bigger issues around combining PSM and DID. For example, PSM matching can actually make DID's parallel trends worse by prioritizing similarity on baseline covariates rather than outcome trends. You might successfully match units with similar characteristics but very different pre-treatment trajectories, violating DiD's core assumption. Another assumption of combining PSM and DID is that there is common support across each period, which is tough to maintain and often untested! The doubly robust approach is also valuable because PSM assumes you "know" the right specification and have the relevant confounders in your model, but this is so so tough. 
